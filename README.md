# üöÄ Custom CLIP: High-Performance Vision-Language Model

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Model-blue)](https://huggingface.co/YOUR_HF_USERNAME/YOUR_MODEL_NAME)
[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-orange)](https://pytorch.org/)

> **A highly optimized, scratch-built implementation of the CLIP architecture (ViT-B/16) designed for accelerated inference. This model achieves 2.46x faster speeds than the standard OpenAI implementation on consumer hardware while maintaining competitive accuracy.**

Developed by **Muhammed K√∂se** as an **Academic Research Project**.

---

## üìñ Table of Contents
- [Overview](#-overview)
- [Key Features](#-key-features)
- [Performance Benchmarks](#-performance-benchmarks)
- [Installation](#-installation)
- [Model Weights](#-model-weights)
- [Usage](#-usage)
- [Training Details](#-training-details)
- [License & Citation](#-license--citation)

---

## üßê Overview

This project presents a custom implementation of the **Contrastive Language-Image Pre-training (CLIP)** architecture. Unlike standard implementations, this project focuses on **engineering optimizations** to make Vision-Language Models (VLMs) feasible for real-time applications on edge devices.

By leveraging **PyTorch 2.0 Compilation (JIT)** and **Mixed Precision (FP16)** techniques, the model significantly reduces latency without compromising semantic understanding capabilities.

---

## ‚ú® Key Features

* **From Scratch Implementation:** The Vision Transformer (ViT) and Text Encoder were coded entirely from scratch, allowing for granular control over the architecture.
* **Extreme Optimization:** Optimized for high throughput using `torch.compile` and `autocast (fp16)`.
* **Real-Time Capabilities:** Capable of running at **47+ FPS** on laptop-grade GPUs (RTX 3050 Ti).
* **High Accuracy:** Fine-tuned on the **COCO Dataset**, achieving **97.71%** zero-shot confidence on test benchmarks.
* **Robust Training:** Implements advanced techniques like Label Smoothing, Dropout, and dynamic data augmentation to prevent overfitting.

---

## üìä Performance Benchmarks

Comparison between the standard **OpenAI CLIP (ViT-B/16)** and this **Custom Optimized Model**.

**Hardware Environment:**
* **GPU:** NVIDIA GeForce RTX 3050 Ti (Laptop)
* **Framework:** PyTorch 2.0 (CUDA 11.8)

| Model | Configuration | Latency (ms) | FPS | Speedup | Confidence Score* |
| :--- | :--- | :--- | :--- | :--- | :--- |
| OpenAI CLIP | Standard (FP32) | 52.22 ms | 19.1 | 1.0x | 99.88% |
| **Custom CLIP** | **FP16 + Compile** | **21.20 ms** | **47.2** | **üöÄ 2.46x** | **97.71%** |

*\*Confidence score measured on a zero-shot classification task (Image: Dog, Labels: [cat, dog, car, ship, ball]).*

---

## üì• Installation

1.  **Clone the repository:**
    ```bash
    git clone [https://github.com/YOUR_GITHUB_USERNAME/YOUR_REPO_NAME.git](https://github.com/YOUR_GITHUB_USERNAME/YOUR_REPO_NAME.git)
    cd YOUR_REPO_NAME
    ```

2.  **Install requirements:**
    ```bash
    pip install -r requirements.txt
    ```
    *(Ensure you have PyTorch installed with CUDA support for best performance.)*

---

## üß† Model Weights

Due to GitHub's file size limits, the pre-trained model weights (`best_model.pt`) are hosted on **Hugging Face**.

üëâ **Download Link:** [Hugging Face Model Repository](https://huggingface.co/YOUR_HF_USERNAME/YOUR_MODEL_NAME)

**Instructions:**
1.  Download the `best_model.pt` file from the link above.
2.  Place it in the root directory of this project.

---

## üíª Usage

You can use the model for **Zero-Shot Image Classification** using the provided script.

```python
import torch
from model import CLIP, load_model
from utils import predict

# 1. Configuration
device = "cuda" if torch.cuda.is_available() else "cpu"
model_path = "best_model.pt"

# 2. Load the optimized model
model = load_model(model_path, device=device)

# 3. Define your test image and labels
image_file = "images/test_image.jpg"
text_prompts = [
    "a photo of a cat",
    "a photo of a dog",
    "a photo of a sports car"
]

# 4. Run Inference
if model:
    predict(model, image_file, text_prompts)
```

## üèãÔ∏è‚Äç‚ôÇÔ∏è Training Details
**Dataset:** COCO 2017 (Common Objects in Context)

**Loss Function:** Symmetric Cross-Entropy with Label Smoothing (0.1)

**Optimizer:** AdamW with Cosine Annealing Learning Rate

## üìú License & Citation
**Source Code**
The source code in this repository is licensed under the MIT License. You are free to use, modify, and distribute it with proper attribution.

**Model Weights**
The pre-trained weights hosted on Hugging Face are licensed under CC-BY 4.0.

**Citation:**
If you use this work in your research or project, please cite it as follows:

```bibtex
@misc{kose2026customclip,
  author = {K√∂se, Muhammed},
  title = {Custom CLIP: Optimized Vision-Language Model},
  year = {2026},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/MuhammedKsee/custom-clip-vit-b-coco}}
}
